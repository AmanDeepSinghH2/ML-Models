{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\amand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('10744.4.txt', 'r', encoding='ISO-8859-1') as file:\n",
    "    content = file.read()\n",
    "\n",
    "with open('positive-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "\n",
    "with open('negative-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    negative_words = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stopwords = set(stopwords.words('english')) | positive_words | negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin replac human futur ?',\n",
       " 'introduct technolog take us ?',\n",
       " 'take leav , technolog alway creat new job much deplet job .',\n",
       " 'might notic certain job disappear job job transform human robot , machin , technolog creat machin replac them.â technolog creat data analysi tool creat custom scenario use artifici ( ai ) , big data machin learn ( ml ) algorithm predict drive consum behavior .',\n",
       " 'data analyt tool , googl analyt , other today , , use , help organ save million , mayb billion dollar sale market .',\n",
       " 'machin replac human ?',\n",
       " 'go , think itâ\\x80\\x99 level set constitutesâ machines.â context articl , machin describ comput computer equip , robot , program learn , sometim human .',\n",
       " 'occasion call artifici ( ai ) , time call machin learn , still time call robot .',\n",
       " 'yes , technic differ thing .',\n",
       " 'bot human specif domain grow pass day .',\n",
       " 'realli task consid human .',\n",
       " 'â , within broad discuss relat futur , total interrel .',\n",
       " 'factori floor deploy robot increas driven machin learn algorithm adjust peopl work alongsid .',\n",
       " 'machin data inform impart daili .',\n",
       " 'everi forward step & advanc technolog , prolifer , replac front .',\n",
       " 'everi pass day seal job human globe .',\n",
       " 'similar , ai use turn hand-drawn sketch ( done human ) digit sourc code .',\n",
       " 'role machin compani futur : compani develop ai robot expertis idea technolog innov theyâ\\x80\\x99ll abl cut costsincreas efficienciesoff new valu propositionsexecut new busi model .',\n",
       " 'cours , itâ\\x80\\x99 machin creativ work togeth either .',\n",
       " 'anoth exampl , amazon employ 100,000 robot warehous move thing around increas warehous workforc 80,000 .',\n",
       " 'human , amazonâ\\x80\\x99 case , pick pack good robot move order around giant warehous , essenti cut â\\x80\\x9cdown walk requir worker , make amazon picker less tired.â\\x80\\x9d plus , robot â\\x80\\x9callow amazon pack shelv togeth car rush-hour traffic longer need aisl space human .',\n",
       " 'greater densiti shelf space mean inventori one roof , mean select customers.â\\x80\\x9d machin replac human ?',\n",
       " 'next decad ( mayb sooner ) , notion whether handl human virtual hing predict .',\n",
       " 'start today , machin manag routin human take â\\x80\\x93 task requir creativ , problem-solv , .',\n",
       " 'context , robot seen mean oper effici also qualiti life worker .',\n",
       " 'although obvious human factor involv activ impact job autom , also true high tasksâ\\x80\\x94and even mechan onesâ\\x80\\x94ar robot .',\n",
       " 'besid greater effici speed , autom lower accid , greater control autonomi , , fewer cost organ .',\n",
       " 'machin human ?',\n",
       " 'although artifici machin learn make us believ robot endow , fact donâ\\x80\\x99t yet abil learn experi respond unknown .',\n",
       " 'thing stand , howev much process speed automat learn robot , doesnâ\\x80\\x99t beat factor innat human brain .',\n",
       " 'human still essenti part process .',\n",
       " 'think deliv servic client .',\n",
       " 'custom challeng routin , human play role address new , solv first time appear , consolid process system .',\n",
       " 'machin vs .',\n",
       " 'human â\\x80\\x93 premium ?',\n",
       " 'machin human place proxim , â robot , doesnâ\\x80\\x99t appli type , especi base robot process autom ( rpa ) , develop process incorpor algorithm signific reduc cost .',\n",
       " 'moreov , think domest robotsâ\\x80\\x94b vacuum , lawn mower pool cleanerâ\\x80\\x94ar increas part daili live .',\n",
       " 'level consumpt robot attain make autom task home obtain greater control , secur .',\n",
       " 'man machin togeth divis human machin â\\x80\\x93 iâ\\x80\\x99m , machin â\\x80\\x93 boundari get fuzzier .',\n",
       " 'prosthet fuse seamless bodi , make limb provid addit strength , , resili , seen exoskeleton don assembl line worker .',\n",
       " 'use smartphon symbiot , direct bodi ?',\n",
       " 'think smartphon form contact len transpar deliv augment realiti imag straight brain .',\n",
       " 'think sound scienc ?',\n",
       " 'think .',\n",
       " 'first prototyp alreadi built .',\n",
       " 'soon , brain-comput interfac could becom , creat new synergist relationship us .',\n",
       " 'point , question know would ; ask question know answer .',\n",
       " 'sometim answer store neural circuitri , time would come connect neuron web .',\n",
       " 'brainâ\\x80\\x99 decis process influenc way â\\x80\\x9ceducatedâ\\x80\\x9d cultur context .',\n",
       " 'extern factor influenc decis process point certain situat , legitim claim influenc brain canâ\\x80\\x99t held account choic made .',\n",
       " 'point iâ\\x80\\x99m tri make human symbiosi cultur environ tool â\\x80\\x93 physic conceptu â\\x80\\x93 taught use .',\n",
       " 'guess transform subtl .',\n",
       " 'conclus practic speak , robot grow point take world start creat , robot even .',\n",
       " 'none expect near futur , long shot .',\n",
       " 'youâ\\x80\\x99v atm , wait pc boot , game x box reach checkpoint , understand world machin everyth .',\n",
       " 'take job , need abl theirsâ\\x80\\x99 ; , depend human lives.â isnâ\\x80\\x99t win-or-los situat .',\n",
       " 'weâ\\x80\\x99r go wind partner machin , partnership foster augment technolog .',\n",
       " 'machin play essenti role augment , technolog , level percept .',\n",
       " 'end , revolut .',\n",
       " 'blackcoff insight 29 : swapna .',\n",
       " 'g , nivashiniya .',\n",
       " 'r , sri manakula vinayagar engin colleg ( smvec ) , puducherri']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word.lower() not in combined_stopwords]\n",
    "    processed_sentences.append(' '.join(filtered_words))\n",
    "\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 3.359843073593074\n",
      "Negative Score: 1.36875\n",
      "Average Sentence Length: 12.96774193548387\n",
      "Percent of Complex Words: 12.189054726368159\n",
      "Fog Index: 10.062718664740812\n",
      "Complex Word Count: 98\n",
      "Total Word Count: 804\n",
      "Personal Pronoun Count: 3\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import syllapy\n",
    "\n",
    "personal_pronouns = {'i', 'me', 'my', 'we', 'our', 'ours', 'us', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'they', 'them', 'their', 'theirs'}\n",
    "total_words = complex_word_count = personal_pronoun_count = positive_score = negative_score = 0\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    total_words += len(words)\n",
    "    complex_word_count += sum(1 for word in words if syllapy.count(word) >= 3)\n",
    "    personal_pronoun_count += sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "    \n",
    "    sentiment = TextBlob(sentence).sentiment.polarity\n",
    "    positive_score += max(sentiment, 0)\n",
    "    negative_score += abs(min(sentiment, 0))\n",
    "\n",
    "avg_sentence_length = total_words / len(processed_sentences)\n",
    "percent_complex_words = (complex_word_count / total_words) * 100\n",
    "fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "\n",
    "print(\"Positive Score:\", positive_score)\n",
    "print(\"Negative Score:\", negative_score)\n",
    "print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "print(\"Percent of Complex Words:\", percent_complex_words)\n",
    "print(\"Fog Index:\", fog_index)\n",
    "print(\"Complex Word Count:\", complex_word_count)\n",
    "print(\"Total Word Count:\", total_words)\n",
    "print(\"Personal Pronoun Count:\", personal_pronoun_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
