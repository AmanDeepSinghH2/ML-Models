{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\amand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('11206.2.txt', 'r', encoding='ISO-8859-1') as file:\n",
    "    content = file.read()\n",
    "\n",
    "with open('positive-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "\n",
    "with open('negative-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    negative_words = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stopwords = set(stopwords.words('english')) | positive_words | negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human machin evolv togeth ?',\n",
       " 'futur upcom year human machin go togeth everi field .',\n",
       " 'upcom day machin need everi human .',\n",
       " 'machin [ ai technolog ] human .',\n",
       " 'machin partner co-oper human .',\n",
       " 'accord professor univers washington , explain , result ai , demand exist job new job creat today .',\n",
       " 'human worker machin togeth , complement .',\n",
       " 'machin learn carri task follow process crunch data .',\n",
       " 'also help human .',\n",
       " 'machin ai creat job opportun human futur .',\n",
       " 'john kelli , execut presid ibm said â\\x80\\x9cman machin work togeth alway beat make decis man machin independently.â\\x80\\x9d â futur , three sector countri agricultur sector , industri sector servic sector go util machin .',\n",
       " ', becom .',\n",
       " ', see agricultur purpos various kind machin use call farm method .',\n",
       " 'major technolog [ machin ] harvest autom , tractor , seed , wee .',\n",
       " 'result , farm agricultur .',\n",
       " 'industri sector also human machin work togeth increas product .',\n",
       " 'various type machin use industri pack machin , load machin etc .',\n",
       " 'human provid instruct machin maintain manag compani .',\n",
       " 'soon robot [ machin ] assist doctor surgeri .',\n",
       " 'instanc , doctor remot locat could direct surgic robot perform open heart surgeri .',\n",
       " 'approach option decis left experi doctor robot .',\n",
       " 'think machin make human less field ?',\n",
       " 'machin push human profession skillset ladder uniqu human skill creativ , social abil , , sense-mak , machin autom .',\n",
       " 'result , machin make workplac , less human .',\n",
       " 'howev , human learn new skill throughout live .',\n",
       " 'said futur 80 % process-ori task done machin .',\n",
       " 'quantit reason task done approxim 50 % human 50 % machin , human continu 80 % cross-funct reason task .',\n",
       " 'accord harvard research machin , algorithm read diagnost scan 92 % accuraci .',\n",
       " 'human 96 % accuraci .',\n",
       " 'togeth , 99 % .',\n",
       " 'human-machin collabor enabl compani interact employe custom novel , way .',\n",
       " 'machin human expand abil three way .',\n",
       " 'amplifi cognit strength ; interact custom employe us higher-level task , embodi human skill extend physic capabl .',\n",
       " 'research , found 1,500 compani achiev perform human machin togeth .',\n",
       " 'new machin system beyond-human cognit abil , mani us could potenti futur .',\n",
       " 'machin inde autom physic task , part quantit task program even data scienc .',\n",
       " 'accord d.e shaw group professor univers washington , explain , result machin , demand exist job , new job creat today .',\n",
       " 'similar couldnâ\\x80\\x99t imagin web app develop decad ago , million make live today .',\n",
       " 'machin task speed , precis , accuraci .',\n",
       " 'machin respond situat make judgment .',\n",
       " 'part left human .',\n",
       " 'henc , need human machin futur .',\n",
       " 'human machin set , combin transform way .',\n",
       " 'machin alreadi infiltr everi aspect live , must learn live .',\n",
       " 'futur , human worker interact close humans.â â â â â â â â â blackcoff insight 29 : vachika sharma , anand english medium school â â â â â â â â â â â â â â â â']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word.lower() not in combined_stopwords]\n",
    "    processed_sentences.append(' '.join(filtered_words))\n",
    "\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 1.1281565656565657\n",
      "Negative Score: 0.16666666666666666\n",
      "Average Sentence Length: 11.177777777777777\n",
      "Percent of Complex Words: 10.934393638170974\n",
      "Fog Index: 8.844868566379501\n",
      "Complex Word Count: 55\n",
      "Total Word Count: 503\n",
      "Personal Pronoun Count: 2\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import syllapy\n",
    "\n",
    "personal_pronouns = {'i', 'me', 'my', 'we', 'our', 'ours', 'us', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'they', 'them', 'their', 'theirs'}\n",
    "total_words = complex_word_count = personal_pronoun_count = positive_score = negative_score = 0\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    total_words += len(words)\n",
    "    complex_word_count += sum(1 for word in words if syllapy.count(word) >= 3)\n",
    "    personal_pronoun_count += sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "    \n",
    "    sentiment = TextBlob(sentence).sentiment.polarity\n",
    "    positive_score += max(sentiment, 0)\n",
    "    negative_score += abs(min(sentiment, 0))\n",
    "\n",
    "avg_sentence_length = total_words / len(processed_sentences)\n",
    "percent_complex_words = (complex_word_count / total_words) * 100\n",
    "fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "\n",
    "print(\"Positive Score:\", positive_score)\n",
    "print(\"Negative Score:\", negative_score)\n",
    "print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "print(\"Percent of Complex Words:\", percent_complex_words)\n",
    "print(\"Fog Index:\", fog_index)\n",
    "print(\"Complex Word Count:\", complex_word_count)\n",
    "print(\"Total Word Count:\", total_words)\n",
    "print(\"Personal Pronoun Count:\", personal_pronoun_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
